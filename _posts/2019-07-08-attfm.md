---
layout: single
title:  "Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks (KR)"
header:
  teaser: "images/syleeie/2019-07-08/AFM.png"
  overlay_color: "#000"
  overlay_filter: "0.5"
  overlay_image: images/syleeie/2019-07-08/attention.png
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
excerpt: "오늘 리뷰할 논문은 Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks Review 입니다."    
categories: 
  - Paper Review
tags:
  - Recommender System
  - Attentional Factorization Machine
  - CTR Prediction
author: syleeie

toc: true
toc_label: "목차"
toc_icon: "cog"

---


## 참고자료 

*   원 논문 (Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks)
    *   [Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks](https://arxiv.org/abs/1708.04617)


## **들어가기 전에 알아야될 용어**

* [Factorization Machines](https://dos-tacos.github.io/paper%20review/FFM/)
    *   FM은 고차원 데이터에서도 제품 변수 간의 상호작용을 통해 효율적으로 사용할 수 있는 지도학습으로 회귀분석과 분류분석 기법을 모두 할 수 있는 기계학습
    *   다항식 회귀 또는 커널 방법과 동등하지만, 더 작고 빠른 모델 평가를 통해 정확도를 얻을 수 있음

    ![](/images/syleeie/2018-12-04/image0_10.png)


* [deepCTR 라이브러리](https://deepctr.readthedocs.io/en/latest/models/DeepModels.html)



## **ABSTRACT**

- Factorization Machines(FM)은 2 차 피쳐 상호 작용을 통합하여 선형회귀 모델을 향상시키는 지도학습 방식
- FM은 모든 기능의 상호 작용이 똑같이 유용하고 예측력을 향상시키는 것이 아니므로 모든 기능 상호 작용을 동일한 가중치로 모델링함으로써 방해받을 수 있습니다. 
    * 예를 들어 쓸모없는 기능과의 상호 작용은 노이즈를 발생시키고 성능을 저하시킬 수 있음
    * 이 작업에서는 서로 다른 기능 상호 작용의 중요성을 식별하여 FM 모델을 향상시킴

- 각각의 피쳐 상호 작용의 중요성을 배우는 AFM (Attentional Factorization Machine)이라는 새로운 모델을 제안
- 신경망 네트워크를 통해 데이터로부터 2 개의 실제 데이터 세트에 대한 광범위한 실험을 통해 AFM의 효율성을 입증. 경험적으로 AFM은 FM이 8.6%의 상대적 향상을 보였고, 최첨단 딥러닝 학습 방법인 Wide & Deep [Cheng et al., 2016]과 DeepCross [Shan et al., 2016]보다 훨씬 간단한 구조와 더 적은 모델 매개 변수를 제공
- AFM 구현은 https://github.com/hexiangnan/attentional_factorization_machine 에서 사용 가능


## **1\. INTRODUCTION**

- 지도학습은 기계학습(ML) 및 데이터마이닝의 기본 작업 중 하나. 목표는 예측변수(일명, 특징)를 입력으로 목표를 예측하는 함수를 추론하는 것 (추천시스템, 온라인 광고, 이미지 인식 등)
- 범주형 예측 변수에 대한 지도학습을 수행할 때 이들 간 상호 작용을 설명하는 것이 중요함 
- 예를 들어, 1) 직업 = {은행가, 엔지니어, ...}, 2) 레벨 = {junior, senior}, 3) 성별 = {남성 여성}, 3가지 카테고리 변수로 고객의 수입을 예측하는 장난감 문제를 고려하기
- junior 뱅커는 junior 엔지니어보다 소득이 낮지만 senior 뱅커는 senior 엔지니어보다 소득이 높음
- ML 모델이 예측변수 사이의 독립성을 가정하고 상호작용을 무시하면 각 피쳐의 가중치를 연결하는 선형 회귀와 같이 정확하게 예측할 수 없으며 모든 피쳐의 가중치 합으로 목표를 예측함
- 피쳐 간의 상호 작용을 활용하기 위한 하나의 공통된 해결책은 피연산자(교차 피쳐)로 벡터를 명시적으로 증가시키는 것. 다차 회귀(PR)에서처럼 각 교차 피쳐의 가중치도 배웁니다. 
- 그러나 PR (그리고 Wide & Deep [Cheng et al., 2016]의 넓은 구성 요소와 같은 다른 유사한 교차 형상기반 솔루션)의 핵심 문제는 단지 몇 개의 교차 피쳐가 관찰되는 희소 데이터 세트의 경우, 관찰되지 않은 형상은 추정 될 수 없음

- PR의 일반화 문제를 해결하기 위해 교차 피처의 가중치를 구성 피처의 포함 벡터의 내적으로 매개변수화 하는 인수분해 장치(FM)[Rendle, 2010]가 제안되었습니다. 
- FM이 동일한 가중치를 갖는 모든 인수 분해된 상호 작용의 모델링에 의해 방해받을 수 있다고 주장. 실제 응용 프로그램에서 서로 다른 예측 변수는 일반적으로 예측력이 다르며 모든 기능에 이전 예제처럼 고객 수입을 예측하는 성별 변수와 같이 대상을 예측하는데 유용한 신호가 들어있는 것은 아님
- 덜 유용한 피쳐와의 상호 작용은 예측에 덜 기여하므로 더 낮은 가중치를 할당해야 함. 그럼에도 불구하고 FM은 기능 상호 작용의 중요성을 차별화하는 기능이 부족하여 예측이 차선적일 수 있음

- 이 작업에서는 피쳐 상호작용의 중요성을 식별하여 FM을 향상시캄. AFM이라는 새로운 모델을 고안함. 신경망 모델링에서 Attention Mechanism을 이용함. 특징 상호 작용이 예측과 다르게 기여할 수 있게함. 더 중요한 것은 피쳐 인터랙션의 중요성이 사람의 도메인 지식 없이 데이터에서 자동으로 학습된다는 것
- 상황인식 예측과 개인화된 태그 추천이라는 두가지 공개 벤치마크 데이터 세트에 대한 실험을 수행함
- 폭넓은 실험을 통해 FM에 대한 관심이 두 가지 이점을 제공한다는 것을 보여주었습니다. 
- 성능 향상뿐 아니라 기능 상호 작용이 예측에 더 많은 기여를 하는지에 대한 통찰력을 제공, FM의 해석 가능성과 투명성을 크게 향상시켜 behavior가 깊은 분석을 수행 할 수 있도록 함


## 2. Factorization Machines

- 지도학습을 위한 일반적인 ML 모델로서, 인수 분해 기계는 원래 collaborative recommendation를 위해 제안되었음
- 실제 가치있는 특징 벡터 x ∈ Rn이 주어지면, n은 피처의 수를 나타내고, FM은 피처의 각 쌍 사이의 모든 상호 작용을 모델링하여 목표를 추정합니다. 
- w0는 전역 바이어스이고, wi는 i번째 피쳐의 가중치이며, wij는 교차 특징 xixj의 가중치를 나타내며, wij = vTivj로 분해된다. 
- vi ∈ Rk는 특징 i에 대한 임베딩 벡터이고, k는 임베딩 벡터의 크기이다. 계수 xi xj로 인해, 0이 아닌 피처 간의 상호 작용만 고려됨

<img src="/images/syleeie/2019-07-08/fm1.png" width="500"> 

- FM 모델은 모두 동일한 방식으로 상호 작용을 특징으로 한다는 점에 주목할 필요가 있음
    * 첫째, 잠재적 벡터 vi는 i번째 기능이 포함하는 모든 기능 상호작용을 추정하는 데 공유됨
    * 둘째, 추정된 모든 특징 상호작용 (wij)은 같은 균일한 가중치를 갖음. 실제로, 모든 특징이 예측과 관련되는 것은 아니다. 
    * 예를 들어, "미국이 해외 지불 투명성에 주도적인 역할을 계속하고 있다"는 문장으로 뉴스 분류 문제를 생각해보십시오. 
    * "외국 지불 투명성" 이외의 단어는 (금융) 뉴스의 주제를 나타내는 것이 아니라는 것은 명백합니다. 관련없는 기능과 상호작용을 고려할 수 있습니다.
- 예측에 아무런 기여도 없는 Feature는 소음이지만 FM은 동일한 가중치로 가능한 모든 기능 상호작용을 모델링하므로 일반화 성능을 저하시킬 수 있음


## 3. Attentional Factorization Machines

### **3.1 Model**

<img src="/images/syleeie/2019-07-08/fm2.png" width="500"> 


- 그림 1은 제안된 AFM 모델의 신경망 구조를 보여줍니다. 그림에서 선형 회귀 분석을 생략. 입력 레이어와 임베디드 레이어는 입력 피쳐에 대해 희소 표현을 채택하고 각각의 0이 아닌 피쳐를 고밀도 벡터에 포함시키는 FM과 동일합니다. 
- 논문의 주된 부분인 pair-wise interaction layer와 attention-based pooling layer에 대해 자세히 설명합니다.

#### Pair-wise Interaction Layer

- inner product을 사용하여 각 피쳐 간 상호 작용을 모델링하는 FM에 영감을 받아 신경망 모델링에서 새로운 Pairwise Interaction Layer를 제안합니다. 

- m 벡터를 m(m - 1)/2 상호 작용 벡터로 확장합니다. 각 상호 작용 벡터는 상호 작용을 인코딩하는 두 개의 별개 벡터의 요소 별 결과입니다. 
- 형식적으로, 특징 벡터 x에서 0이 아닌 피처 집합을 X라고 하고, 임베디드 계층의 출력을 E = {vixi} i∈X 라고 하면, 쌍 - 쌍방향 상호 작용 층의 출력을 집합은 다음과 같음

<img src="/images/syleeie/2019-07-08/fm3.png" width="300"> 

- 두 벡터의 element-wise 곱을 나타내며, pair-wise interaction layer를 정의함으로써 신경망 네트워크 구조 하에서 FM을 표현할 수 있음
- Sum Pooling을 사용하여 pair-wise 레이어를 압축한 다음 fully connected layer를 사용하여 예측 점수에 투영
- p와 b는 각 예측 레이어의 가중치와 바이어스를 나타냅니다. 확실히 p를 1로 고정하고 b를 0으로 설정하면 FM 모델을 정확하게 복구할 수 있습니다. 
- 최근 신경망 FM의 연구는 Bilinear Interaction 풀링 연산를 제안했는데, pair-wise 상호작용 계층에서 sum 풀링을 사용하는 것으로 볼 수 있음


#### Attention-based Pooling Layer

- Attention Mechanism은 신경망 모델링에 도입되었기 때문에 추천, 정보 검색, 컴퓨터 시각에서 아이디어는 여러 부분을 단일 표현으로 압축할 때 다르게 기여하도록 허용하는 것입니다.
- FM 모델의 단점에 동기를 두어 상호작용 벡터에 대해 가중합을 수행하여 특징 상호 작용에 Attention Mechansim을 적용할 것을 제안함
- aij는 특징 상호 작용 wij에 대한 Attention 점수이며 wij는 다음을 예측하는 데 중요한 요소로 해석될 수 있음

<img src="/images/syleeie/2019-07-08/fm4.png" width="300"> 

- 기술적으로 예측 손실을 최소화하여 직접 학습하는 직관적인 솔루션인 aij를 평가함
- 그러나 문제는 훈련 데이터에서 결코 공동 발생하지 않은 기능의 경우 상호작용의 Attention 점수를 예측할 수 없다는 것
- 일반화 문제를 해결하기 위해 Attention Network 에서 MLP (multi-layer perceptron)를 사용하여 Attention 점수를 매개 변수화 함
- Attention Network에 대한 입력은 상호작용 정보를 포함하여 영역에 인코딩하는 두 가지 기능의 상호 작용 벡터입니다.
- 공식적으로, Attention Network는 다음과 같이 정의

<img src="/images/syleeie/2019-07-08/fm5.png" width="300"> 

- t(모델 파라미터)는 Attention Network의 숨겨진 계층 크기를 나타냅니다. Attention 점수는 softmax 기능을 통해 정상화됩니다. 
- rectifier 함수를 활성화 기능으로 사용함 (경험적으로 좋은 성능을 보여줌)
- Attention 기반 풀링 계층의 출력은 k차원 벡터
- 이 벡터는 중요도를 구분하여 임베딩 공간에서 모든 피쳐 상호 작용을 압축하여 다음 이를 예측 점수에 투영함
- 요약하면, 우리는 AFM 모델의 전체 공식을 다음과 같이 제시

<img src="/images/syleeie/2019-07-08/fm6.png" width="300"> 


### **3.2 Learning**

- AFM은 데이터 모델링의 관점에서 FM을 직접 강화하므로 회귀분석, 분류 및 순위 지정 등 다양한 예측 작업에도 적용할 수 있음 
- 다른 목적을 위해서는 AFM 모델 학습하려면 다른 목적 함수가 사용되어야 함
- 목표 y(x)가 실수값인 회귀 작업의 경우 공통 목적 함수는 제곱 된 손실입니다. 여기서 T는 학습 인스턴스 집합을 나타냅니다. 묵시적 피드백 [He et al., 2017b]에 의한 바이너리 분류 또는 추천 작업을 위해 우리는 로그 손실을 최소화 할 수있다. 이 논문에서는 회귀 과제에 중점을두고 제곱 손실을 최적화합니다.

목적 함수를 최적화하기 위해 우리는 확률 적 구배 강하 (SGD) - 신경망 모델을위한 범용 해법을 사용합니다. SGD 알고리즘을 구현하는 핵심은 예측 모델 yAFM (x) w.r.t의 미분을 얻는 것입니다. 각 매개 변수. 깊은 학습을위한 대부분의 최신 툴킷이 Theano 및 TensorFlow와 같은 자동 차별화 기능을 제공하므로 여기에서 파생 상품의 세부 정보는 생략합니다.

피할 수없는 예방
오버 피팅은 ML 모델을 최적화 할 때 영구적 인 문제입니다. FM은 overfitting [Rendle et al., 2011]에 시달릴 수 있으므로 L2 정규화는 FM에 대한 과잉 방지를 방지하는 필수 요소입니다. AFM은 FM보다 표현력이 강하기 때문에 훈련 데이터를 너무 많이 맞추는 것이 더 쉽습니다. 여기서 우리는 overfitting (dropout 및 L2 regularization)을 방지하는 두 가지 기술을 고려했습니다.
널리 신경망 모델에 사용됩니다.

중도 탈락의 개념은 훈련 중 (Srivastava et al., 2014) 일부 신경을 (연결에 따라) 무작위로 떨어 뜨리는 것이다. 이것은 훈련 데이터에 대한 뉴런의 복잡한 동시 적응을 예방할 수있는 것으로 나타났습니다. AFM은 모든 상호 작용이 유용하지는 않지만 피쳐 간의 모든 쌍 방향 상호 작용을 모델링하기 때문에 쌍 - 쌍방향 상호 작용 계층의 뉴런은 서로 쉽게 상호 적응하여 초과 적합을 초래할 수 있습니다. 따라서 우리는 공동 적응을 피하기 위해 쌍방향 상호 작용 계층에서 드롭 아웃을 사용합니다. 또한 드롭 아웃은
테스트 및 전체 네트워크가 예측에 사용되는 경우 드롭 아웃은 잠재적으로 성능을 향상시킬 수있는 더 작은 신경망으로 모델 평균을 수행하는 또 다른 부작용을 가지고 있습니다 [Srivastava et al., 2014].

하나의 계층 MLP 인 관심 네트워크 구성 요소에 대해 가능한 승복을 방지하기 위해 가중치 행렬 W에 L2 정규화를 적용합니다. 즉, 우리가 최적화하는 실제 목적 함수는 다음과 같습니다.



### 2.1 DeepFM

- 저차원 및 고차원 기능 상호 작용을 배우는 것을 목표로 Factorization-Machine based neural network(DeepFM)을 제안
- 그림에 묘사된 것처럼 DeepFM은 동일한 입력을 공유하는 FM 구성 요소와 딥러닝 구성 요소의 두 가지 요소로 되어 있음
    - 기능 i의 경우 스칼라 wi가 순서의 중요성을 측정하는 데 사용됨, 잠재 벡터 Vi는 다른 기능과의 상호 작용의 영향을 측정하는 데 사용됩니다. 
    - Vi는 FM 구성 요소에서 order2 피쳐 상호 작용을 모델링하고 고차 피쳐 상호 작용을 모델링하기 위해 깊은 구성 요소로 구성됨
    - wi, Vi 및 네트워크 매개 변수를 포함한 모든 매개 변수는 결합된 예측 모델을 위해 훈련됨. 여기서 y (0, 1)는 예측된 CTR이고 yFM은 FM 구성 요소의 출력이며 yDNN은 깊은 구성 요소의 출력

<img src="/images/syleeie/2019-04-02/20190402_2.png" width="400"> 

- FM 구성 요소는 [Rendle, 2010] 추천 시스템을 위해서 기능 상호 작용을 배우기 위해 제안된 factorization machine
    - 기능 간의 선형 (차수 1) 상호 작용 외에도 FM 모델은 쌍방향 (차수 2) 상호 작용을 각각의 피쳐 잠재 벡터의 내부 곱으로 특징으로 함

 <img src="/images/syleeie/2019-04-02/20190402_3.png" width="700">   

- 데이터 세트가 희박할 때 이전 접근법보다 훨씬 효과적으로 주문 order 2 Feature 상호 작용을 캡처할 수 있음
    - 이전 접근법에서 피쳐 i와 피쳐 j의 상호 작용 매개 변수는 피쳐 i와 피쳐 j가 모두 동일한 데이터 레코드에 나타날 때만 훈련 될 수 있음
    - FM에서 latent 벡터 Vi와 Vj의 내부 곱을 통해 측정됨. 이 유연한 디자인 덕분에 FM은 데이터 레코드에 i (또는 j)가 나타날 때마다 잠재 벡터 Vi (Vj)를 훈련시킬 수 있음
    - 따라서 훈련 데이터에 나타나지 않거나 거의 나타나지 않는 기능 상호 작용은 FM에서 더 잘 학습됩니다.
    - 그림 2가 보여 주듯이, FM의 출력은 추가 단위와 다수의 내부 제품 단위의 합

<img src="/images/syleeie/2019-04-02/20190402_4.png" width="700"> 

- 모든 수치에서 검정색의 정규 연결은 학습할 weight와의 연결을 의미함. 
    - 파란색 대시 화살표는 학습할 잠재 벡터를 의미
    - 장치의 출력은 두 개의 입력 벡터의 곱. 함수는 CTR 예측에서 출력 함수로 사용됨. relu 및 tanh와 같은 활성화 함수는 신호를 비선형으로 변환하는 데 사용됨

<img src="/images/syleeie/2019-04-02/20190402_5.png" width="700"> 

- DNN 요소는 피드 포워드 신경망으로 고차 기능 상호 작용을 배우는 데 사용됨
    - 그림 3에서 볼 수 있듯이 데이터 레코드(벡터)가 신경망에 학습. 순전히 연속적이고 밀도가 높은 입력으로 이미지 [He et al., 2016] 또는 오디오 [Boulanger-Lewandowski et al., 2013] 데이터를 가진 신경망과 비교할 때 CTR 예측의 입력은 매우 다르므로 새로운 네트워크 아키텍처 설계가 필요합니다. 
    - 특히, CTR 예측을 위한 원시 피쳐 입력 벡터는 일반적으로 매우 희박하고, 초고차원, 범주형 연속 혼합 및 필드 (예 : 성별, 위치, 연령)로 그룹화됨
    - 입력 벡터를 첫 번째 숨겨진 레이어로 더 인풋하기 전에 저차원의 고밀도 실수 값 벡터로 압축하기 위한 임베딩 레이어를 제안
    - 그림 4는 입력 계층에서 임베딩 계층까지 하위 네트워크 구조를 강조 표시함

<img src="/images/syleeie/2019-04-02/20190402_6.png" width="700"> 

- 이 네트워크 구조의 두 가지 흥미로운 특징
    - 1) 서로 다른 입력 필드 벡터의 길이가 다를 수 있지만 임베딩은 동일한 크기 (k)
    - 2) FM의 잠재 피쳐 벡터 (V)는 이제 서버에서 입력 필드 벡터를 임베딩 벡터로 압축하는 데 사용되는 네트워크 가중치로 사용됩니다. [Zhang et al., 2016]
        - V는 FM에 의해 사전 훈련되고 초기화로 사용됨

<img src="/images/syleeie/2019-04-02/20190402_7.png" width="700"> 

- 작업에서 [Zhang et al., 2016]에서와 같이 네트워크를 초기화하기 위해 FM의 잠재 피쳐 벡터를 사용하는 대신 다른 DNN 모델 외에도 전반적인 학습 아키텍처의 일부로 FM 모델을 포함합니다
    - 따라서 FM에 의한 사전 훈련의 필요성을 제거하고 대신 전체 네트워크를 End to End 방식으로 훈련시킴 

- FM 구성 요소와 딥 구성 요소는 동일한 기능 임베딩을 공유하므로 두 가지 중요한 이점을 제공
    - 1) 원시 기능에서 low 및 high 기능 상호 작용을 모두 배웁니다. 
    - 2) Wide & Deep [Cheng et al., 2016]에서 요구되는대로 입력의 전문가의 기능 엔지니어링이 필요하지 않음


### 2.2 Relationship with the other Neural Networks

- 다양한 응용 분야에서의 딥러닝 학습의 엄청난 성공에서 영감을 얻은 CTR 예측을 위한 몇 가지 딥러닝 모델이 최근에 개발되었음
- 해당 섹션에서는 제안된 DeepFM을 CTR 예측을 위한 기존 딥러닝 모델과 비교
    - FNN : 그림 5 (왼쪽)가 보여 주듯이, FNN은 FM 초기화된 피드 포워드 신경망 [Zhang et al., 2016]입니다. 

- FM 사전 훈련 전략은 두 가지 제한을 초래함    
    1) 임베딩 매개 변수가 FM의 영향을 받을 수 있음 
    2) 사전 훈련 단계에서 도입된 오버 헤드에 의해 효율성이 감소됨
    - 또한 FNN은 고차 피쳐 상호 작용만 캡처

- 대조적으로 DeepFM은 사전 훈련을 필요로 하지 않으며 고차 및 저차 기능 상호 작용을 모두 학습
    - PNN : 고차 피쳐 상호 작용을 포착하기 위해 PNN은 임베딩 레이어와 첫 번째 숨겨진 레이어 [Qu et al., 2016] 사이에 Product 레이어 추가. 
    - PNN (다양한 유형의 제품 작동)에 따르면 IPNN, OPNN 및 PNN의 세 가지 변형이 있음
    - IPNN은 벡터의 Inner 곱을 기반으로 하며 OPNN은 Outer 곱을 기반으로 하며 PNN은 Inner 및 Outer 곱을 기반으로 함

<img src="/images/syleeie/2019-04-02/20190402_8.png" width="700"> 

- 계산을 보다 효율적으로 하기 위해 저자는 Inner Product과 Outer Product의 근사 계산을 제안함

<img src="/images/syleeie/2019-04-02/20190402_9.png" width="700"> 

-  1) Inner Product은 일부 뉴런을 제거하여 대략 계산
-  2) Outer Product은 m k-차원 피쳐 벡터를 하나의 k 차원 벡터로 압축하여 대략 계산
    - Outer Product의 근사 계산은 결과를 불안정 하게 만드는 많은 정보를 잃어 버리기 때문에 Outer Product이 Inner Product 보다 신뢰성이 낮다는 것을 알게됨
    - Inner Product이 더 신뢰할 만하지만, output of the product layer이 첫 번째 숨겨진 층의 모든 뉴런에 연결되어 있기 때문에 여전히 높은 계산 복잡성을 겪음

- PNN과 달리 DeepFM의 Product layer 출력은 최종 출력 계층(하나의 뉴런)에만 연결됨
    - FNN과 마찬가지로 모든 PNN은 low-order 피쳐 상호 작용을 무시



#### Wide & Deep 

- Google에서 Low 및 High 기능 상호 작용을 동시에 모델링하기 위해 제안됨 
- "Wide" 부분에 대한 입력에 대한 전문가의 기능 엔지니어링이 필요로 함 (예 : 앱 권장 사항의 사용자 설치 앱 및 인상 앱의 교차 제품)
- 대조적으로, DeepFM은 입력 원시 기능에서 직접 학습하여 입력을 처리할 수 있는 전문가 지식이 필요하지 않음
    - 이 모델의 직접적인 확장은 LR을 FM으로 대체하는 것 (3 절에서이 확장을 평가합니다
    - 이 확장은 DeepFM과 유사하지만 DeepFM은 FM과 DNN 구성 요소 사이에 피쳐를 임베딩
    - 피쳐 임베딩 영향 (역전파 방식으로)의 공유 전략은 표현을 보다 정확하게 모델링 하는 low 피쳐 상호 작용에 의한 피쳐 표현을 제공

- 요약하면 DeepFM과 다른 Deep learning 모델 간의 관계는 표 1에 나와있음
    - 보다시피 DeepFM은 사전 훈련 및 피쳐 엔지니어링이 필요없는 유일한 모델이며 low 및 High 피쳐 상호 작용을 모두 캡처


## 3. Experiments

- 이 섹션에서는 제안된 DeepFM과 다른 최첨단 모델을 경험적으로 비교
    - 평가 결과는 제안된 DeepFM이 다른 최첨단 모델보다 효과적이며 DeepFM의 효율성은 다른 모델보다 우수한 모델과 유사하다는 것을 나타냄


### 3.1 Experiment Setup Datasets 

- 다음 두 데이터 세트에서 제안된 DeepFM의 효율성과 효율성을 평가함


- 1) Criteo Dataset : Criteo 데이터 세트 5에는 4 천 5 백만 명의 사용자 클릭 기록이 포함됨
    - 13개의 연속적인 특징과 26개의 범주형 특징
    - 데이터 세트를 무작위로 두 부분으로 나눔. 90%는 훈련용이고 나머지 10% 는 테스트용
    
- 2)  Company * Dataset 세트
    - 실제 산업 CTR 예측에서 DeepFM의 성능을 확인하기 위해 회사 데이터 세트에 대한 실험을 수행
    - 컴퍼니 앱 스토어의 게임 센터에서 7일 연속 사용자 클릭 기록을 수집하고 다음 날에는 테스트를 위해 수집
    - 수집된 전체 데이터 세트에는 약 10억 개의 기록이 있음
    - 데이터 세트에는 앱 기능 (예 : 식별, 범주 등), 사용자 기능 (예 : 사용자의 다운로드 된 앱 등) 및 컨텍스트 기능 (예 : 작동 시간 등)이 있습니다. 


- 평가 측정법
    - 실험에서 AUC (ROC 넓이)와 Logloss (교차 엔트로피)의 두 가지 평가 메트릭을 사용합
    - 모델 비교 우리는 LR, FM, FNN, PNN (3 가지 변형), Wide & Deep 및 DeepFM과 같은 실험에서 9 가지 모델을 비교합니다. 
    - Wide & Deep 모델에서는 피쳐 엔지니어링 노력을 없애기 위해 LR을 FM으로 대체하여 원래의 와이드 앤 딥 모델을 넓은 부분으로 채택
    - Wide & Deep의 두 변종된 모델을 구별하기 위해 각각 LR&DNN과 FM&DNN이라는 이름을 붙임

- 매개변수 설정 
    - Criteo 데이터 세트의 모델을 평가하기 위해 FNN 및 PNN의 매개 변수 설정을 따름
    - (1) dropout 0.5, (2) network structure 400-400-400
    - (3) optimizer Adam (4) activation function IPNNN의 경우 tanh, 다른 딥러닝 모델의 경우 relu
    - 공정하게 말하자면, 제안된 DeepFM은 동일한 설정을 사용
    - LR과 FM의 optimizer는 각각 FTRL과 Adam이며 FM의 잠재 차원은 10입니다. 
    - 회사 데이터 세트의 각 개별 모델에 대한 최상의 성능을 달성하기 위해 3.3절에서 논의한 신중한 매개변수 연구를 수행
    

### 3.2 Performance Evaluation

- 두 데이터 세트의 섹션 3.1에 나열된 모델을 평가하여 효율성과 효율성을 비교, 딥러닝 학습 모델의 효율성은 실제 응용 프로그램에 중요함
- Criteo 데이터 세트의 다른 모델의 효율성을 다음과 같은 공식으로 비교함 (ㅣtraining time of deep CT R modelㅣ / ㅣ training time of LRㅣ)
- 결과는 CPU (왼쪽) 및 GPU (오른쪽)에 대한 테스트를 포함하여 그림 6에 나와있음
    - 1) FNN의 사전 훈련으로 인해 효율성이 떨어짐
    - 2) IPNN 및 PNN의 속도가 다른 모델보다 높지만 GPU의 속도는 여전히 계산 비용이 많이 듦, 비효율적인 inner 제품 작업
    - 3) 가장 효율적인 DeepFM을 달성

<img src="/images/syleeie/2019-04-02/20190402_10.png" width="700"> 

- Criteo 데이터 세트 및 회사 데이터 세트에 대한 CTR 예측 성능은 표 2에 나와 있음
    - 학습 기능 상호 작용은 CTR 예측 모델의 성능을 향상시킴. 이 관찰은 LR (기능 상호 작용을 고려하지 않는 유일한 모델)이 다른 모델보다 성능이 저하된다는 사실에서 비롯된 것
    - 최고의 모델로서 DeepFM은 회사 및 Criteo 데이터 세트에서 AUC (Logloss 측면에서 1.15 % 및 5.60 %) 측면에서 LR을 0.86 % 및 4.18 % 능가
    - 고차 및 저차 피쳐 상호 작용을 동시에 배우고 CTR 예측 모델의 성능을 적절하게 향상시킵니다

- DeepFM은 저차 피쳐 상호 작용 (즉, FM) 또는 고차 피쳐 상호 작용 (즉, FNN, IPNN, OPNN, PNN)만을 배우는 모델보다 성능이 뛰어납니다. 
    - 두 번째로 좋은 모델과 비교하여 DeepFM은 회사 및 Criteo 데이터 세트에서 AUC (Logloss 측면에서 0.42 % 및 0.29 %) 측면에서 0.37 % 이상과 0.25 % 이상을 달성합니다. 
    - 고차 및 저차 기능 상호 작용을 동시에 학습하면서 고차 및 저차 기능 상호 작용을 위해 동일한 기능을 공유하면 CTR 예측 모델의 성능이 향상됩니다.

 <img src="/images/syleeie/2019-04-02/20190402_11.png" width="700">    

- DeepFM은 별도의 기능 임베딩(LR & DNN 및 FM &amp; DNN)을 사용하여 고차 및 저차 기능 상호 작용을 배우는 모델보다 성능이 뛰어납니다. 
    - 이 두 모델과 비교하여 DeepFM은 회사 및 Criteo 데이터 세트에서 AUC (Logloss 측면에서 0.61 % 및 0.66 %) 측면에서 0.48 % 이상 및 0.33 % 이상을 달성합니다. 
    - 전반적으로, 제안된 DeepFM 모델은 AUC 및 Logloss on Company 데이터 세트 측면에서 각각 0.37 % 및 0.42 % 이상 경쟁 모델을 이김
    - 사실 오프라인 AUC 평가가 약간 개선되면 온라인 CTR이 크게 증가할 수 있음 
    - [Cheng et al., 2016]에서 보고한 바와 같이 LR과 비교하여 Wide & Deep은 AUC를 0.275 % (오프라인) 향상시키고 온라인 CTR의 개선은 3.9 %입니다. 
    - 컴퍼니 앱스토어의 일일 매출은 수백만 달러이므로 CTR의 리프트는 매년 수백만 달러를 추가로 제공함


### 3.3 Hyper-Parameter Study

- 다양한 딥러닝 모델의 다양한 하이퍼 매개 변수가 회사 데이터 세트에 미치는 영향을 연구
- 순서는 다음과 같음

    - 1) 활성화 기능
    - 2) Dropout
    - 3) number of neurons per layer
    - 4) number of hidden layers
    - 5) network shape
    
- [Qu et al., 2016]에 따르면, relu와 tanh는 시그모이드보다 Deep Learning 모델에 더 적합함
    - 이 논문에서 relu와 tanh를 적용할 때 딥 모델의 성능을 비교합니다. 
    - 그림 7에서 볼 수 있듯이 relu는 IPNN을 제외한 모든 딥 모델에 대해 tanh보다 적합합니다. 가능한 이유는 Relu 함수가 희박함을 유도한다는 것

<img src="/images/syleeie/2019-04-02/20190402_12.png" width="700">    


- Dropout [Srivastava et al., 2014]는 뉴런이 네트워크에 유지 될 확률을 나타냄
    - Dropout은 신경망의 정밀도와 복잡성을 손상시키는 정규화 기술
    - Dropout를 1.0, 0.9, 0.8, 0.7, 0.6, 0.5로 설정했다. 그림 8에서 볼 수 있듯이, 모든 모델은 Dropout이 적절하게 설정되면 (0.6에서 0.9까지) 최고 성능에 도달할 수 있음
    - 결과 모델에 합리적인 무작위성을 추가하면 모델의 견고성이 강화될 수 있음을 보여줌

 <img src="/images/syleeie/2019-04-02/20190402_13.png" width="700">    

- 다른 요인들이 동일하게 유지될 때, 층당 뉴런의 수를 증가시키는 것은 복잡성을 초래
    - 그림 9에서 볼 수 있듯이 뉴런의 수를 늘리면 항상 이득을 얻을 수는 없음
    - 예를 들어, DeepFM은 층 당 뉴런 수가 400에서 800으로 증가할 때 안정적으로 수행됨
    - 더 나쁜 것은 OPNN이 뉴런 수를 400에서 800으로 늘릴 때 더 나쁜 결과를 나타냄, 지나치게 복잡한 모델이 과도하게 적합하기 쉽기 때문. 데이터 세트에서 레이어당 200개나 400개의 뉴런이 좋은 선택

 <img src="/images/syleeie/2019-04-02/20190402_14.png" width="700"> 

- 그림 10에 제시된 숨겨진 계층의 수는 숨겨진 계층의 수가 증가함에 따라 처음에는 모델의 성능이 향상되지만 숨겨진 계층의 수가 계속 증가하면 성능이 저하. 현상은 또한 과적합 때문에 일어난다

 <img src="/images/syleeie/2019-04-02/20190402_15.png" width="700"> 

- 네트워크 형상은  constant, increasing, decreasing, diamond 등 네 가지 네트워크 모양을 테스트
    - 네트워크 모양을 변경하면 숨겨진 레이어의 수와 뉴런의 총 수를 수정
    - 예를 들어, 숨겨진 층의 수가 3이고 뉴런의 총 수가 600일 때 상수 (200-200-200), 증가 (100-200-300), 감소 (300-200-100), 다이아몬드 (150-300- 150)
    - 그림 11에서 볼 수 있듯이, 'constant' 네트워크 형태는 이전 연구와 일치하는 다른 세 가지 선택사항보다 경험적으로 더 좋음 

 <img src="/images/syleeie/2019-04-02/20190402_16.png" width="700">       


## 4. Related Work

- 논문에서 CTR 예측을 위해 새로운 Deep learning이 제안됨. 가장 관련있는 도메인은 추천 시스템에서 CTR 예측 및 Deep learning
- 1절과 2.2절에서는 CTR 예측을 위한 몇 가지 깊은 학습 모델이 이미 언급되어 있기 때문에 그에 대해 논의하지 않음
- CTR 예측 이외의 추천시스템 과제에는 몇 가지 Deep learning 모델이 제안되어 있음
    - 예를 들어, [Covington et al., 2016; Salakhutdinov et al., 2007; van den Oord et al., 2013; Wu et al., 2016; Zheng et al., 2016; Wu et al., 2017; Zheng et al., 2017). [Salakhutdinov et al., 2007; Sedhain et al., 2015; Wang et al., 2015]는 딥러닝 학습을 통해 협업 필터링을 개선할 것을 제안
    - [Wang and Wang, 2014; van den Oord et al., 2013]는 음악 추천의 성능을 향상시키기 위해 딥러닝 학습을 통해 컨텐츠 기능을 추출
    - [Chen et al., 2016]은 이미지 기능과 디스플레이 광고의 기본 기능을 모두 고려하기 위해 딥러닝 네트워크를 고안 
    - [Covington et al., 2016]은 YouTube 비디오 추천을 위한 2 단계 딥러닝 학습 프레임 워크를 개발


## 5. Conclusions

- 이 논문에서 CTR 예측을 위한 Factorization-Machine 기반 신경망인 DeepFM을 제안하여 최첨단 모델의 단점을 극복하고 더 나은 성능을 달성했음
- DeepFM은 딥러닝 구성 요소와 FM 구성 요소를 공동으로 학습 
    - 1) 사전 훈련이 필요하지 않습니다
    - 2) 고차 및 저차 기능 상호 작용을 모두 배웁니다
    - 3) 피쳐 엔지니어링을 피하기 위해 피쳐 임베딩의 공유 전략을 소개합니다. 
    
- Deep FM과 최첨단 모델의 효율성과 효율성을 비교하기 위해 두 개의 실제 데이터 세트(크리테오 데이터셋과 상업용 앱 스토어 데이터 세트)에 대한 광범위한 실험을 실시
    - 1) DeepFM이 두 데이터 세트에서 AUC 및 Logloss 측면에서 최첨단 모델보다 성능이 뛰어났다는 것을 보여줍니다. 
    - 2) DeepFM의 효율성은 최첨단에서 가장 효율적인 딥 모델과 비교할 수 있습니다

- 미래 연구를 위한 두 가지 흥미로운 방향이 있음
    - 하나는 가장 유용한 고차원 기능 상호 작용을 학습하는 능력을 강화하기 위해 일부 전략 (예 : 풀링 레이어를 도입하는 것)을 탐구하는 것
    - 다른 하나는 대규모 문제를 위해 GPU 클러스터에서 DeepFM을 훈련시키는 것